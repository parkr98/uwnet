<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Deep Learning - 04 - Convolutional Neural Networks</title>
    <style>
        foreignObject {overflow:visible}
        img {max-width:100%;
        vertical-align:middle;}
        /*figure {max-width:100%}*/
        .small {font-size:70%}
        .large {font-size:130%}
        /*.mermaidsvg {max-width:40%;}*/
        .remark-container figure{
            margin:0;
        }

        .col30{float:left; width:30%}
        .col40{float:left; width:40%}
        .col50{float:left; width:50%}
        .col60{float:left; width:60%}
        .col70{float:left; width:70%}



        /*@import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
        @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
        @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

        body { font-family: 'Droid Serif'; }
        h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
        }
        .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
        */
    </style>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="https://www.desmos.com/api/v1.5/calculator.js?apiKey=dcb31709b452b1cf9dc26972add0fda6"></script>
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>

  </head>
  <body>
    <h1 id="chapter-4-convolutional-neural-networks">Chapter 4: Convolutional Neural Networks</h1>

<h2 id="fully-connected-layers-and-images">Fully Connected Layers and Images</h2>

<p>Say we want to run our neural networks on image data. We know that in image processing, feature extraction is very important to make machine learning models work well.</p>

<p>One standard technique for extracting features is the <strong>histogram of oriented gradients (HOG)</strong> method. You don’t have to worry too much about how HOG works, it’s basically just looking at patches in the input image and measuring which way the gradients (edges) face in those patches. The important part is the dimensions of HOG features. HOG features produce a 36-element vector for each \(8 \x 8\) patch in the input.</p>

<p>Say we want to learn feature extraction using neural networks instead. If we want to mimic HOG features our network might look like:</p>
<ul>
  <li>Input: \(256 \x 256 \x 3\) RGB image</li>
  <li>Hidden: \(32 \x 32 \x 36\) feature map</li>
  <li>Output: \(1000\) classes</li>
</ul>

<p>Since fully connected layers are <em>fully</em> connected there are 7.2 billion connections between the input and hidden layer!</p>

<p>During feature extraction, our network is looking at the entire image every time it computes a single feature. However, HOG features (and most image features) are meant to capture local information, like what edges are present nearby. We don’t need to look at the whole image for this.</p>

<p>For most feature extraction on images we can impose the assumption that pixels that are far apart are statistically independent. This means we only need to look at smaller, local regions when performing feature extraction.</p>

<h2 id="convolutions-powerful-local-feature-extraction">Convolutions: Powerful, Local Feature Extraction</h2>

<p><strong>Convolutions</strong> are a powerful tool for local feature extraction in a signal. The convolution operation takes two functions and slides one along the other, performing a dot product of their intersection to form an output signal. Note: in computer vision we talk about convolutions but in the signal processing domain what we’re actually talking about is a cross-correlation. I’m not going to try to correct a decade of mistaken terminology in this class so I’m just going to call it a convolution idk sorry…</p>

<p>Anyway, it’s nice to start with a 1-D example. We’ll be talking about discrete functions which we can just think of as vectors. We’ll define the convolution operator as follows:</p>

\[(m * f)(x) = m_{[x-1, x+1]} \cdot f\]

<h3 id="example-convolution-high-pass-filter">Example Convolution: High-Pass Filter</h3>

<p>Take discrete functions \(m\) and \(f\). To convolve these filters we simply slide the filter \(f\) along input signal (or function) \(m\) and take weighted sums, storing the result in a new vector.</p>

<ul>
  <li>Let’s calculate \(m * f\) where
    <ul>
      <li>\(m = [0, 1, 2, 3, 2, 5, 6, 7, 8, 14, 10, 11, 12, 8, 14]\)</li>
      <li>\(f = [-1, 2, -1]\)</li>
      <li>First element is dot product of first three elements of \(m\) with \(f\), \(m_{[0-2]} \cdot f\)</li>
      <li>…</li>
    </ul>
  </li>
</ul>

\[\begin{align*}
&amp;(m*f)(1)  = m_{[0-2]} \cdot f  = [0,1,2] \cdot [-1,2,-1]      =   0 + 2 - 2     =&amp; 0   \\
&amp;(m*f)(2)  = m_{[1-3]} \cdot f  = [1,2,3] \cdot [-1,2,-1]      =  -1 + 4 - 3     =&amp; 0   \\
&amp;(m*f)(3)  = m_{[2-4]} \cdot f  = [2,3,2] \cdot [-1,2,-1]      =  -2 + 6 - 2     =&amp; 2   \\
&amp;(m*f)(4)  = m_{[3-5]} \cdot f  = [3,2,5] \cdot [-1,2,-1]      =  -3 + 4 - 5     =&amp; -4  \\
&amp;(m*f)(5)  = m_{[4-6]} \cdot f  = [2,5,6] \cdot [-1,2,-1]      =  -2 + 10 - 6    =&amp; 2   \\
&amp;(m*f)(6)  = m_{[5-7]} \cdot f  = [5,6,7] \cdot [-1,2,-1]      =  -5 + 12 - 7    =&amp; 0   \\
&amp;(m*f)(7)  = m_{[6-8]} \cdot f  = [6,7,8] \cdot [-1,2,-1]      =  -6 + 14 - 8    =&amp; 0   \\
&amp;(m*f)(8)  = m_{[6-8]} \cdot f  = [7,8,14] \cdot [-1,2,-1]     =  -7 + 16 - 14   =&amp; -5  \\
&amp;(m*f)(9)  = m_{[6-8]} \cdot f  = [8,14,10] \cdot [-1,2,-1]    =  -8 + 28 - 10   =&amp; 10  \\
&amp;(m*f)(10) = m_{[6-8]} \cdot f  = [14,10,11] \cdot [-1,2,-1]   =  -14 + 20 - 11  =&amp; -5  \\
&amp;(m*f)(11) = m_{[6-8]} \cdot f  = [10,11,12] \cdot [-1,2,-1]   =  -10 + 22 - 12  =&amp; 0   \\
&amp;(m*f)(12) = m_{[6-8]} \cdot f  = [11,12,8] \cdot [-1,2,-1]    =  -11 + 24 - 8   =&amp; 5   \\
&amp;(m*f)(13) = m_{[6-8]} \cdot f  = [12,8,14] \cdot [-1,2,-1]    =  -12 + 16 - 14  =&amp; -10 \\
\end{align*}\]

<p>So the output is the vector:</p>

\[m*f = [0 , 0 , 2 , -4 , 2 , 0 , 0 , -5 , 10 , -5 , 0 , 5 , -10]\]

<p>This particular filter, \(f = [-1, 2, -1]\), is a <strong>high-pass filter</strong>. It doesn’t respond to slow changes in the function but does respond to fast changes. The results is an output signal centered around zero that oscillates up and down when the input signal quickly moves up or down.</p>

<figure class="figure">
<iframe src="https://www.desmos.com/calculator/6tm5gtkaxs?embed" width="500px" height="500px" style="border: 1px solid #ccc" frameborder="0"></iframe>
<figcaption>Input signal \(m\) and output \(m*f\). The high-pass filter suppresses the slow rise of \(m\) but shows the wiggles.</figcaption>
</figure>

<h3 id="padding">Padding</h3>

<p>Notice that our input signal \(m\) has 15 elements but our output \(m*f\) only has 13 elements. We can’t apply the filter outside the bounds of our original function \(m\) so our output signal \(m*f\) with have size \(\vert m\vert - \lfloor\frac{\vert f \vert}{2}\rfloor\), in this case \(15 - 2 = 13\). Often it is helpful to have your input and output be the same size (like when processing data with neural networks!) so you may want to <strong>pad</strong> your input with data to make the output end up the same size. Consider using:</p>

<ul>
  <li><strong>zero-padding</strong>: pad with zeros</li>
  <li><strong>clamp-padding</strong> (extend/replicate padding): repeat the last elements outward</li>
  <li><strong>wrap-padding</strong> (circular padding): wrap around to the other side of the input signal</li>
  <li><strong>mirror-padding</strong> (reflect padding): reflect data from inside the signal outward into padded region</li>
</ul>

<p>Most of the time zero-padding is a good option but you can try other versions too.</p>

<h3 id="low-pass-filter">Low-Pass Filter</h3>

<p>Another useful filter to know is \(f = [0.\overline{3}, 0.\overline{3}, 0.\overline{3}]\) or \(f = [0.2, 0.2, 0.2, 0.2, 0.2]\) or in general \(f = \frac{1}{n}[1,1,1,\dots,1]\). This is a <strong>low-pass filter</strong> or blurring filter, the output at every pixel is an average of a local window in the input signal.</p>

<h3 id="convolutions-in-2d">Convolutions In 2D</h3>

<p>Convolutional filters are powerful feature extractors in 2D. Here’s an example of high-pass and low-pass filters (or <strong>box filter</strong>) in two dimensions:</p>

\[\text{High-Pass Filter} = \begin{bmatrix}
-1 &amp; -1 &amp; -1 \\
-1 &amp;  8 &amp; -1 \\
-1 &amp; -1 &amp; -1 \\
\end{bmatrix}\]

\[\text{Low-Pass "Box" Filter} = \frac{1}{9}\begin{bmatrix}
1 &amp; 1 &amp; 1 \\
1 &amp; 1 &amp; 1 \\
1 &amp; 1 &amp; 1 \\
\end{bmatrix}\]

<h4 id="output-of-high-pass-filter-on-image">Output of High-Pass Filter on Image</h4>

<p><img src="../figs/dog.jpg" alt="An image of a dog on a porch with a bike, a car in the background" /> \(*\begin{bmatrix}
-1 &amp; -1 &amp; -1 \\
-1 &amp;  8 &amp; -1 \\
-1 &amp; -1 &amp; -1 \\
\end{bmatrix} =\)
<img src="../figs/dog-highpass.jpg" alt="A mostly black image with lines in the original image in white" /></p>

<h4 id="output-of-low-pass-filter-on-image">Output of Low-Pass Filter on Image</h4>

<p><img src="../figs/dog.jpg" alt="An image of a dog on a porch with a bike, a car in the background" /> \(* \frac{1}{9}\begin{bmatrix}
1 &amp; 1 &amp; 1 \\
1 &amp; 1 &amp; 1 \\
1 &amp; 1 &amp; 1 \\
\end{bmatrix} =\)
<img src="../figs/dog-box7.jpg" alt="A blurry version of the original image" /></p>

<h4 id="sobel-filters">Sobel Filters</h4>

<p>Sobel filters are convolutional filters designed to take the (approximate) derivative of an image in the x and y dimension.</p>

\[G_x = \begin{bmatrix}
-1 &amp;  0 &amp;  1 \\
-2 &amp;  0 &amp;  2 \\
-1 &amp;  0 &amp;  1 \\
\end{bmatrix} =\]

\[G_y = \begin{bmatrix}
-1 &amp; -2 &amp; -1 \\
 0 &amp;  0 &amp;  0 \\
 1 &amp;  2 &amp;  1 \\
\end{bmatrix} =\]

<p>Using these filters we can do things like find high rates of change in the x-dimension (vertical lines), y-dimension (horizontal lines), or calculate the magnitude and direction of all gradients (lines) in the image.</p>

<p><img src="../figs/dog.jpg" alt="An image of a dog on a porch with a bike, a car in the background" /> \(* \begin{bmatrix}
-1 &amp;  0 &amp;  1 \\
-2 &amp;  0 &amp;  2 \\
-1 &amp;  0 &amp;  1 \\
\end{bmatrix} =\)
<img src="../figs/dog-dx.jpg" alt="A mostly black image with vertical lines in the original image drawn in white" /></p>

<p><img src="../figs/dog.jpg" alt="An image of a dog on a porch with a bike, a car in the background" /> \(* \begin{bmatrix}
-1 &amp; -2 &amp; -1 \\
 0 &amp;  0 &amp;  0 \\
 1 &amp;  2 &amp;  1 \\
\end{bmatrix} =\)
<img src="../figs/dog-dy.jpg" alt="A mostly black image with horizontal lines in the original image drawn in white" /></p>

<p>\(\arctan(\frac{dy}{dx}) =\)<img src="../figs/dog-color.jpg" alt="A mostly black image lines in the original image drawn in color, the color depending on the angle of the edge in the original image" /></p>

<h2 id="convolutions-as-feature-extraction">Convolutions as Feature Extraction</h2>

<p>Hopefully you are now convinced that convolutional filters are effective at extracting useful information from local patches in signal data (such as images). Though they are powerful feature extractors, they are relatively simple, we are just doing a weighted sum of the input with a filter.</p>

<p>Convolutions, especially filters like sobel filters, were commonly used as a first step in feature extraction before deep neural networks. The features they found would be aggregated in local areas to produce descriptors like HOG features.</p>

<p>But now we want to bake feature extraction into our neural network. Fully connected layers were too dense but convolutions are spare operations (each pixel in the output is only connected to a local neighborhood in the input) and are computationally fairly cheap (like connected layers we are just doing a weighted sum to compute convolutions).</p>

<p>So instead of using pre-defined convolutional filters we can implement neural network layers that perform convolution operations. The weights in these layers can be updated via error backpropagation so we learn filters that work well for the particular task.</p>

<h2 id="convolutional-layer">Convolutional Layer</h2>

<ul>
  <li>Input: An image</li>
  <li>Processing: Convolution with multiple filters</li>
  <li>Output: An image/feature map of extracted features</li>
</ul>

<p>Parameters:</p>
<ul>
  <li>number of filters: each filter produces a feature map in the output, the number of filters determines the number of channels in the output</li>
  <li>filter size: spatial dimensions of the filters</li>
  <li>stride: how far the filter moves with every application</li>
</ul>

<h3 id="number-of-filters">Number of Filters</h3>

<p>Each filter processes the input image or feature map and produces a single channel for the output feature map. More filters mean the layer can specialize and extract more specific features. However it also means the layer take longer to process.</p>

<p>In general you need fewer features in the early layers because there aren’t very many interesting features to look for. For instance in the first layer there are only so many orientations of edges and color blobs to look for. Adding more filters can just lead to redundancy without being productive.</p>

<p>Later layers tend to have more filters as the spatial dimensions of the image shrinks. There’s less information in the spatial dimension but more semantic information and more channels to process and look for patterns in.</p>

<h3 id="filter-size">Filter Size</h3>

<p>Larger filters can process more information in the spatial dimensions but take longer to process. Smaller filters are faster. In general filters are square and usually have odd parity in size. Only a few filters are commonly used although there is room to explore other options. The common ones are:</p>

<ul>
  <li>\(3 \times 3\) filter: the most common filter size, it can process information in the spatial dimension but also is very efficient and fast</li>
  <li>\(1 \times 1\) filter: second most common size, it doesn’t process spatial information but does find patterns across channels and is very fast</li>
  <li>\(7 \times 7\) filter: often larger size filters are used in the first layer where the cost is much less since there are only 3 channels in the input</li>
  <li>\(2 \times 2\) filter, stride of 2: not very common but one alternative to maxpooling layers for downsampling, although \(3 \times 3\) stride 2 convolutions are more common</li>
</ul>

<h3 id="stride">Stride</h3>

<p>Typically convolutional filters are applied at every pixel location.</p>




<script>
mermaid.initialize({
    startOnLoad:false,
    cloneCssStyles: false,
    flowchart:{
        curve:'basis',
        htmlLabels:true,
    },
    theme: 'default',
    themeVariables:{
        edgeLabelBackground:'#fff',
    },
});

window.onload = function(){
            mermaid.init();

            window.MathJax = {
              tex: {
                macros: {
                  d: "{\\nabla}",
                  x: "{\\times}",
                }
              }
            };

            (function () {
              var script = document.createElement('script');
              script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
              script.async = true;
              document.head.appendChild(script);
            })();
    
};
</script>

  </body>
</html>
